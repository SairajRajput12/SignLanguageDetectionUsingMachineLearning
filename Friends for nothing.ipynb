{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08188d87-298d-48be-a69a-e464796296c1",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e6d53c5-76bb-4299-8e23-8491ec89d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense , Dropout\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe084fb9-99ee-4350-a8a8-c1debd74bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 128 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562f9e57-0cd6-459c-a96f-7997cbf3457b",
   "metadata": {},
   "source": [
    "# Initialising our first Convolution network model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8356268-2817-4331-8713-01aea8d974a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e769855-742d-458e-bda9-b0eab1d1241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convolution network and pooling\n",
    "\n",
    "\n",
    "# First convolution layer and pooling\n",
    "classifier.add(Convolution2D(32, (3, 3), input_shape=(sz, sz, 1), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Second convolution layer and pooling\n",
    "classifier.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "# input_shape is going to be the pooled feature maps from the previous convolution layer\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#classifier.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "# input_shape is going to be the pooled feature maps from the previous convolution layer\n",
    "#classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flattening the layers\n",
    "classifier.add(Flatten())\n",
    "\n",
    "\n",
    "# Adding a fully connected layer\n",
    "classifier.add(Dense(units=128, activation='relu'))\n",
    "classifier.add(Dropout(0.40))\n",
    "classifier.add(Dense(units=96, activation='relu'))\n",
    "classifier.add(Dropout(0.40))\n",
    "classifier.add(Dense(units=64, activation='relu'))\n",
    "classifier.add(Dense(units=27, activation='softmax')) # softmax for more than 2\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # categorical_crossentropy for more than 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f278cc57-d0bf-4957-aa22-0b49fecab482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 126, 126, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 63, 63, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 61, 61, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 30, 30, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 28800)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               3686528   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 96)                12384     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                6208      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 27)                1755      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3716443 (14.18 MB)\n",
      "Trainable params: 3716443 (14.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fa1f837-bb85-41a2-808c-50b3d9adc8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3914b4f1-d107-4ef9-b248-9b8358b5a014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 648 images belonging to 27 classes.\n",
      "Found 162 images belonging to 27 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('ProcessedData128/train',\n",
    "                                                 target_size=(sz, sz),\n",
    "                                                 batch_size=10,\n",
    "                                                 color_mode='grayscale',\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('ProcessedData128/test',\n",
    "                                            target_size=(sz , sz),\n",
    "                                            batch_size=10,\n",
    "                                            color_mode='grayscale',\n",
    "                                            class_mode='categorical') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "220c10da-9545-4b59-b81e-5ad7e37dc994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "65/65 [==============================] - 7s 103ms/step - loss: 0.0577 - accuracy: 0.9815 - val_loss: 0.0965 - val_accuracy: 0.9693\n",
      "Epoch 2/15\n",
      "65/65 [==============================] - 7s 102ms/step - loss: 0.0879 - accuracy: 0.9691 - val_loss: 0.1062 - val_accuracy: 0.9693\n",
      "Epoch 3/15\n",
      "65/65 [==============================] - 7s 102ms/step - loss: 0.0777 - accuracy: 0.9753 - val_loss: 0.1003 - val_accuracy: 0.9755\n",
      "Epoch 4/15\n",
      "65/65 [==============================] - 7s 105ms/step - loss: 0.0854 - accuracy: 0.9738 - val_loss: 0.0627 - val_accuracy: 0.9693\n",
      "Epoch 5/15\n",
      "65/65 [==============================] - 8s 117ms/step - loss: 0.1178 - accuracy: 0.9676 - val_loss: 0.0630 - val_accuracy: 0.9755\n",
      "Epoch 6/15\n",
      "65/65 [==============================] - 7s 101ms/step - loss: 0.0815 - accuracy: 0.9769 - val_loss: 0.0622 - val_accuracy: 0.9755\n",
      "Epoch 7/15\n",
      "65/65 [==============================] - 6s 99ms/step - loss: 0.0825 - accuracy: 0.9691 - val_loss: 0.1094 - val_accuracy: 0.9693\n",
      "Epoch 8/15\n",
      "65/65 [==============================] - 6s 99ms/step - loss: 0.1006 - accuracy: 0.9660 - val_loss: 0.0787 - val_accuracy: 0.9632\n",
      "Epoch 9/15\n",
      "65/65 [==============================] - 7s 102ms/step - loss: 0.0794 - accuracy: 0.9753 - val_loss: 0.0959 - val_accuracy: 0.9693\n",
      "Epoch 10/15\n",
      "65/65 [==============================] - 7s 102ms/step - loss: 0.0633 - accuracy: 0.9769 - val_loss: 0.0973 - val_accuracy: 0.9693\n",
      "Epoch 11/15\n",
      "65/65 [==============================] - 7s 101ms/step - loss: 0.0764 - accuracy: 0.9815 - val_loss: 0.1280 - val_accuracy: 0.9755\n",
      "Epoch 12/15\n",
      "65/65 [==============================] - 7s 102ms/step - loss: 0.0903 - accuracy: 0.9753 - val_loss: 0.0951 - val_accuracy: 0.9816\n",
      "Epoch 13/15\n",
      "65/65 [==============================] - 7s 100ms/step - loss: 0.0643 - accuracy: 0.9753 - val_loss: 0.1316 - val_accuracy: 0.9632\n",
      "Epoch 14/15\n",
      "65/65 [==============================] - 7s 101ms/step - loss: 0.0594 - accuracy: 0.9799 - val_loss: 0.0894 - val_accuracy: 0.9693\n",
      "Epoch 15/15\n",
      "65/65 [==============================] - 6s 99ms/step - loss: 0.0624 - accuracy: 0.9784 - val_loss: 0.0666 - val_accuracy: 0.9693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x27fe39c4750>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(\n",
    "        training_set,\n",
    "        epochs=15,\n",
    "        verbose=1, \n",
    "        validation_data=test_set,\n",
    ")# No of images in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "22b85d84-68a8-442d-867b-ee67c7b8678a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 29ms/step - loss: 0.0666 - accuracy: 0.9693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06657802313566208, 0.9693251252174377]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d6636d8e-2b6c-45ef-97da-244237ef3841",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'E:/MachineLearning/SignDetectionSystemUsingMachineLearning/models/SignDetection.h5' \n",
    "classifier.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "31368041-5d65-491c-afce-6cacdaf356dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "savedModel=load_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e549be8b-36b6-49f7-96e2-1b533ec7c641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 126, 126, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 63, 63, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 61, 61, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 30, 30, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 28800)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               3686528   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 96)                12384     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                6208      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 27)                1755      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3716443 (14.18 MB)\n",
      "Trainable params: 3716443 (14.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "savedModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "03e6c390-8b7f-430d-b4a0-68ccdcdef7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 17ms/step - loss: 0.0666 - accuracy: 0.9693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06657800823450089, 0.9693251252174377]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savedModel.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b901014-35d3-40d0-aa75-12a04779c33a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93ecf2e-d301-43d8-a55b-de3329e14cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d65bc-4b12-4960-a14a-f4ddc2c2104b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf92b4f8-c13e-4a74-9666-df07ff48d6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab9aacc-6f28-45a0-bb0a-918b78e25900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
